import torch
from torch import nn

class Conv(nn.Module):
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size=(3,3),
                 stride=(1,1),
                 padding=1):
        super().__init__()

        self.conv = nn.Conv2d(in_channels,out_channels,
                              kernel_size,
                              stride,
                              padding)
        self.norm = nn.InstanceNorm2d(out_channels)

        self.act = nn.ReLU()

    def forward(self,x):
        return self.act(self.norm(self.conv(x)))

class ConvT(nn.Module):
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size=(2,2),
                 stride=(2,2),
                 padding=0):
        super().__init__()

        self.conv = nn.ConvTranspose2d(in_channels,out_channels,
                              kernel_size,
                              stride,
                              padding)
        self.norm = nn.InstanceNorm2d(out_channels)

        self.act = nn.ReLU()

    def forward(self,x):
        return self.act(self.norm(self.conv(x)))

class Generator(nn.Module):
    def __init__(self,
                 in_channels,
                 out_channels,
                 n_layers=6):
        super().__init__()

        self.layers = []
        hidden_dim = 64
        for _ in range(n_layers):
            self.layers.append(
                ConvT(in_channels,
                      hidden_dim)
            )
            in_channels = hidden_dim
            hidden_dim = hidden_dim * 2

        self.embedding = nn.Embedding(64, 64)


        self.out_layer = Conv(in_channels,out_channels)
    def forward(self,x,condition):

        for i in range(len(self.layers)):
            x = self.layers[i](x)
            if i == 0:
                x += self.embedding(condition).unsqueeze(-1).unsqueeze(-1)
        return self.out_layer(x)
x = torch.randn(1,100,1,1)
gen = Generator(100,3)
x_out = gen(x, torch.tensor([4]))
x_out.shape

class Discriminator(nn.Module):
    def __init__(self,
                 in_channels,
                 out_channels,
                 n_layers):
        super().__init__()

        self.layers = []
        hidden_dim = 32
        for _ in range(n_layers):
            self.layers.append(
                Conv(
                    in_channels,
                    hidden_dim,
                    (2,2),
                    (2,2),
                    0
                )
            )
            in_channels = hidden_dim
            hidden_dim = hidden_dim * 2

        self.embedding = nn.Embedding(32, 32)

        self.out = Conv(in_channels,
                        out_channels)

    def forward(self,x,condition):
        for i in range(len(self.layers)):
            x = self.layers[i](x)
            if i == 0:
                x = x + self.embedding(condition).unsqueeze(-1).unsqueeze(-1)
        return self.out(x)

x = torch.randn(1,3,512,512)
disc = Discriminator(3,1,3)
disc(x, torch.tensor([0])).shape
