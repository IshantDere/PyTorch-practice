import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

class CNN(nn.Module):
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size=4,
                 stride=2,
                 padding=1):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels,
                      out_channels,
                      kernel_size,
                      stride,
                      padding,
                      bias=True,
                      padding_mode="reflect"),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(0.2)
        )

    def forward(self, x):
        return self.conv(x)

class CNNTranspose(nn.Module):
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size=4,
                 stride=2,
                 padding=1):
        super().__init__()
        self.conv = nn.Sequential(
            nn.ConvTranspose2d(in_channels,
                               out_channels,
                               kernel_size,
                               stride,
                               padding,
                               bias=True),
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
        )

    def forward(self, x):
        return self.conv(x)

class Generator(nn.Module):
    def __init__(self, in_channels=3, features=64):
        super().__init__()
        self.down1 = CNN(in_channels, features)
        self.down2 = CNN(features, features*2)
        self.down3 = CNN(features*2, features*4)
        self.down4 = CNN(features*4, features*8) 

        self.bottleneck = nn.Sequential(
            nn.Conv2d(features*8, features*8, 4, 2, 1),
            nn.ReLU()
        )

        self.up1 = CNNTranspose(features*8, features*8)
        self.up2 = CNNTranspose(features*16, features*4)
        self.up3 = CNNTranspose(features*8, features*2)
        self.up4 = CNNTranspose(features*4, features)

        self.final = nn.Sequential(
            nn.ConvTranspose2d(features*2, in_channels, 4, 2, 1),
            nn.Tanh()
        )

    def forward(self, x):
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)

        bottleneck = self.bottleneck(d4)

        u1 = self.up1(bottleneck)
        u2 = self.up2(torch.cat([u1, d4], dim=1))
        u3 = self.up3(torch.cat([u2, d3], dim=1))
        u4 = self.up4(torch.cat([u3, d2], dim=1))

        return self.final(torch.cat([u4, d1], dim=1))

class Discriminator(nn.Module):
    def __init__(self, in_channels=3):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_channels*2, 64, 4, 2, 1), 
            nn.LeakyReLU(0.2),

            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),

            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2),

            nn.Conv2d(256, 512, 4, 1, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2),

            nn.Conv2d(512, 1, 4, 1, 1)
        )

    def forward(self, x, y):
        return self.net(torch.cat([x, y], dim=1))

def train_pix2pix(dataloader, num_epochs=10, device="cuda"):
    gen = Generator().to(device)
    disc = Discriminator().to(device)

    opt_gen = optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5, 0.999))
    opt_disc = optim.Adam(disc.parameters(), lr=2e-4, betas=(0.5, 0.999))

    adv_criterion = nn.BCEWithLogitsLoss()
    l1_criterion = nn.L1Loss()

    for epoch in range(num_epochs):
        for idx, (x, y) in enumerate(dataloader):
            x, y = x.to(device), y.to(device)

## Training
            fake = gen(x)
            D_real = disc(x, y)
            D_fake = disc(x, fake.detach())

            loss_real = adv_criterion(D_real, torch.ones_like(D_real))
            loss_fake = adv_criterion(D_fake, torch.zeros_like(D_fake))
            loss_disc = (loss_real + loss_fake) / 2

            opt_disc.zero_grad()
            loss_disc.backward()
            opt_disc.step()


            D_fake = disc(x, fake)
            loss_adv = adv_criterion(D_fake, torch.ones_like(D_fake))
            loss_l1 = l1_criterion(fake, y) * 100 
            loss_gen = loss_adv + loss_l1

            opt_gen.zero_grad()
            loss_gen.backward()
            opt_gen.step()

        print(f"Epoch [{epoch+1}/{num_epochs}] "
              f"Loss_D: {loss_disc.item():.4f}, "
              f"Loss_G: {loss_gen.item():.4f}")

    return gen, disc

if __name__ == "__main__":
    x = torch.randn((1, 3, 256, 256))
    y = torch.randn((1, 3, 256, 256))

    gen = Generator()
    disc = Discriminator()

    fake = gen(x)
    disc_out = disc(x, fake)

    print(fake.shape)
    print(disc_out.shape)
